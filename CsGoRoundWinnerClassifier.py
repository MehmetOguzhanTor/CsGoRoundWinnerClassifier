# -*- coding: utf-8 -*-
"""data proje.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mWiQxs_P804aEUTt2Cye4UnVU6yOsRsd
"""

import random
import pandas as pd
import numpy as np
import timeit
import matplotlib.pyplot as plt
from numpy import linalg as LA

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

id = '1kYQG7Ry9g92MKmOogoXuTDZIOZxYL4hy'
downloaded = drive.CreateFile({'id':id})
downloaded.GetContentFile('csgo_round_snapshots.csv')

df = pd.read_csv('csgo_round_snapshots.csv')

# Extracting the weapons that are never taken before
df = df.drop(['ct_weapon_bizon', 'ct_weapon_g3sg1', 't_weapon_m249', 'ct_weapon_negev', 'ct_weapon_r8revolver', 'ct_weapon_sawedoff'], axis=1)
df.describe().transpose()

# Looking for attributes
column_names = df.columns
column_names

# If CT won label = 1 and T  won label = 0
cleanup_nums = {"map":     {"de_dust2": 1, "de_inferno": 2, "de_mirage": 3, "de_nuke":4,
                            "de_overpass":5, "de_vertigo":6, "de_train":7, "de_cache": 8},
                "round_winner": {"CT": 1, "T":0}}
df = df.replace(cleanup_nums)
df["bomb_planted"]= df["bomb_planted"].astype(float)

# Deleting a line with faulty data
df = df[df['t_health']!=600]

# First 16 attributions of the dataset
data = df[column_names[0:16]]
# Shuffling the dataset
data = data.sample(frac=1)
# Taking the first 1000 rows of the shuffled dataset
data = data.head(1000)

# Since the dataset consists of continuous information, It should be shuffled. This shuffling happens every time the code is running.
data_all_np=df.values.astype('float')
random.seed(0)
np.random.shuffle(data_all_np)

# Splitting the dataset into test training and verification datasets
data_len = data_all_np.shape[0]
# training set 70%
data_train = data_all_np[:int(data_len*0.7)]
# verification set 10%
data_valid = data_all_np[int(data_len*0.7):int(data_len*0.8)]
# test set 20%
data_test = data_all_np[int(data_len*0.8):]
data_test_lr = data_test

# Splitting the dataset into test training and verification datasets (df)
data_len = df.shape[0]
# training set 70% (df)
data_train_df = df[:int(data_len*0.7)]
# verification set 10%
data_valid_df = df[int(data_len*0.7):int(data_len*0.8)]
# test set 20%
data_test_df = df[int(data_len*0.8):]

# Splitting training set into 2 different matrix as their T winning and CT winning situations
data_t_train = data_train[data_train[:,-1] == 0]
data_ct_train = data_train[data_train[:,-1] == 1]

# Finding the priors of Y = CT which is label being 1(P(Y=CT)) and Y = T which is label being 0(P(Y=T))
prior_t = data_t_train.shape[0]/(data_t_train.shape[0]+data_ct_train.shape[0])
prior_ct = data_ct_train.shape[0]/(data_t_train.shape[0]+data_ct_train.shape[0])

# Splitting the label vectors and the remaining dataset and write them into new matrices
data_t_train_only, data_t_label = data_t_train[:,:-1], data_t_train[:,-1]
data_ct_train_only, data_ct_label = data_ct_train[:,:-1], data_ct_train[:,-1]

# Reshaping the label vectors
data_t_label = data_t_label.reshape(np.size(data_t_label),1)
data_ct_label = data_ct_label.reshape(np.size(data_ct_label),1)

# Normalization function
def normalization(data_np):
  x_max = np.max(data_np,axis=0).reshape(np.size(data_np,1),1)
  x_min = np.min(data_np,axis=0).reshape(np.size(data_np,1),1)
  num = np.subtract(data_np , np.transpose(x_min))
  denum = np.subtract(x_max , x_min)
  normalized_data = (num/np.transpose(denum))
  return normalized_data

# Starting timer for training
start_timer_naive_train = timeit.default_timer()

# Normalizing the T train and CT train matrices
# When the program runs this part, it gives runtime error because of 0 column vectors which we are getting rid of below if there are any
normalized_data_t_train = normalization(data_t_train_only)
normalized_data_ct_train = normalization(data_ct_train_only)

# Using np.mean and np.std to find mean and std of train matrices
mean_vector_data_ct_train = np.mean(normalized_data_ct_train, axis=0, keepdims=True).reshape(np.size(normalized_data_ct_train,1),1)
std_vector_data_ct_train = np.std(normalized_data_ct_train, axis=0).reshape(np.size(normalized_data_ct_train,1),1)
mean_vector_data_t_train = np.mean(normalized_data_t_train, axis=0, keepdims=True).reshape(np.size(normalized_data_t_train,1),1)
std_vector_data_t_train = np.std(normalized_data_t_train, axis=0).reshape(np.size(normalized_data_t_train,1),1)

# Finding the column numbers with nan values
nan_columns_std_ct = list(np.argwhere(np.isnan(std_vector_data_ct_train))[:,0])
nan_columns_std_t = list(np.argwhere(np.isnan(std_vector_data_t_train))[:,0])
nan_columns_mean_t = list(np.argwhere(np.isnan(mean_vector_data_t_train))[:,0])
nan_columns_mean_ct = list(np.argwhere(np.isnan(mean_vector_data_ct_train))[:,0])
nan_list = list(set(nan_columns_std_ct + nan_columns_std_t + nan_columns_mean_t+nan_columns_mean_ct))

# Deleting the columns with nan mean or std values
std_vector_data_ct_train = np.delete(std_vector_data_ct_train, nan_list, axis=0)
std_vector_data_t_train = np.delete(std_vector_data_t_train, nan_list, axis=0)
mean_vector_data_ct_train = np.delete(mean_vector_data_ct_train, nan_list, axis=0)
mean_vector_data_t_train = np.delete(mean_vector_data_t_train, nan_list, axis=0)

data_test = np.delete(data_test, nan_list, axis=1)
n_data_test = normalization(data_test)


# stopping train timer and calculating the run time
#stop_timer_naive_train = timeit.default_timer()
#run_time_naive_train = stop_timer_naive_train - start_timer_naive_train
#print(run_time_naive_train)

"""Naive Bayes Classification

"""

# Getting test labels from the test dataset
test_labels = n_data_test[:,-1]
n_data_test = np.delete(n_data_test, data_test.shape[1]-1, axis=1)

# Starting timer for testing
start_timer_naive_test = timeit.default_timer()

# The function to find the likelihood by using gaussian formula
# The formula uses dataset, mean of the dataset, std of the dataset and the prior knowledge
# We are taking the logarithm of the function to ease the job
# It returns the posterior from the naive bayes formula
def naive_prob_generate(dt, mean_dt, std_dt, apriori):
  exp_num = np.square((dt - mean_dt.T))
  exp_denum = (-2*(np.square(std_dt)))
  exp_input = (exp_num / exp_denum.T)
  log_1 = np.log(1/(std_dt.T*np.sqrt(2*np.pi)))
  gauss_matrix = np.add(log_1,exp_input)
  prob = np.sum(gauss_matrix,axis=1).reshape(dt.shape[0],1)
  prob = prob*apriori
  return prob

# This function calculates confusion matrix values
# then prints Precision, Accuracy, and Recall
def conf_matrix(test_labels, predicted_labels):

  tp, tn, fp, fn = 0, 0, 0, 0
  for i in range(len(test_labels)):
    if test_labels[i] == predicted_labels[i] and test_labels[i] == 1:
      tp += 1
    elif test_labels[i] == predicted_labels[i] and test_labels[i] == 0:
      tn += 1
    elif test_labels[i] != predicted_labels[i] and test_labels[i] == 1:
      fn += 1
    else:
      fp += 1

  precision = tp/(tp + fp+1)
  accuracy = (tp+tn)/(tp+fp+fn+tn)
  recall = tp/(tp+fn)
  print(f"Precision = {100*precision:.2f} %")
  print(f"Accuracy = {100*accuracy:.2f} %")
  print(f"Recall = {100*recall:.2f} %")
  return

# Using the function to calculate the probabilities
ct_prob=naive_prob_generate(n_data_test, mean_vector_data_ct_train, std_vector_data_ct_train, prior_ct)
t_prob=naive_prob_generate(n_data_test, mean_vector_data_t_train, std_vector_data_t_train, prior_t)

# Making comparison to make prediction
ct_predict = ct_prob > t_prob

# Creating the confusion matrix
conf_matrix(test_labels, ct_predict)
# # acc = 0
# true_positive = 0
# true_negative = 0
# false_positive = 0
# false_negative = 0
# for l in range(len(test_labels)):
#   if test_labels[l] == ct_predict[l] and test_labels[l] == 1:
#     acc += 1
#     true_positive += 1
#   elif test_labels[l] == ct_predict[l] and test_labels[l] == 0:
#     acc += 1
#     true_negative += 1
#   elif test_labels[l] != ct_predict[l] and test_labels[l] == 1:
#     false_negative += 1
#   else:
#     false_positive += 1


# # Finding the accuracy
# print(100*(acc/len(test_labels)))

# stopping test timer and calculating the run time
stop_timer_naive_test = timeit.default_timer()
run_time_naive_test = stop_timer_naive_test - start_timer_naive_test
print("The runtime for Naive Bayes model is: ",run_time_naive_test)

"""# Logistic Regression"""

#Starting timer for training
start_timer_logreg_train = timeit.default_timer()

# Seperation of train, test, and validation sets
data_train_logreg, train_label_logreg = data_train[:,:-1], data_train[:,-1]
train_label_logreg = train_label_logreg.reshape(np.size(train_label_logreg),1)
data_test_logreg, test_label_logreg = data_test_lr[:,:-1], data_test_lr[:,-1]
test_label_logreg = test_label_logreg.reshape(np.size(test_label_logreg),1)
data_valid_logreg, valid_label_logreg = data_valid[:,:-1], data_valid[:,-1]
valid_label_logreg = valid_label_logreg.reshape(np.size(valid_label_logreg),1)
# Normalization of datasets (there are nan columns depending on the shuffle)
normalized_train_logreg = normalization(data_train_logreg)
normalized_test_logreg = normalization(data_test_logreg)
normalized_valid_logreg = normalization(data_valid_logreg)
# Finding the nan columns and deleting from all datasets
mean_train_logreg = np.mean(normalized_train_logreg, axis=0, keepdims=True).reshape(np.size(normalized_train_logreg,1),1)
mean_test_logreg = np.mean(normalized_test_logreg, axis=0, keepdims=True).reshape(np.size(normalized_test_logreg,1),1)
mean_valid_logreg = np.mean(normalized_valid_logreg, axis=0, keepdims=True).reshape(np.size(normalized_valid_logreg,1),1)
nan_traincolumns_logreg = list(np.argwhere(np.isnan(mean_train_logreg))[:,0])
nan_testcolumns_logreg = list(np.argwhere(np.isnan(mean_test_logreg))[:,0])
nan_validcolumns_logreg = list(np.argwhere(np.isnan(mean_valid_logreg))[:,0])

nan_list_logreg = list(set(nan_traincolumns_logreg + nan_testcolumns_logreg + nan_validcolumns_logreg))

normalized_train_logreg = np.delete(normalized_train_logreg, nan_list_logreg, axis=1)
normalized_test_logreg = np.delete(normalized_test_logreg, nan_list_logreg, axis=1)
normalized_valid_logreg = np.delete(normalized_valid_logreg, nan_list_logreg, axis=1)
decision_tree_trainset = normalized_train_logreg
# Adding 1 to first column for b0
normalized_train_logreg = np.concatenate( (normalized_train_logreg, np.ones((np.shape(normalized_train_logreg)[0],1))), axis=1)
normalized_test_logreg = np.concatenate( (normalized_test_logreg, np.ones((np.shape(normalized_test_logreg)[0],1))), axis=1)
normalized_valid_logreg = np.concatenate( (normalized_valid_logreg, np.ones((np.shape(normalized_valid_logreg)[0],1))), axis=1)

# Learning rates for validation set
lrn_rate = [0.005, 0.010, 0.025, 0.050, 0.100]
# Epoch Number
epoch_number = 100
accs = np.zeros( (epoch_number, len(lrn_rate)) )
wgh_ite = np.zeros((len(normalized_train_logreg[0]),len(lrn_rate)))
gra_ite = np.zeros((len(normalized_train_logreg[0]),len(lrn_rate)))
train_repeat = np.repeat(train_label_logreg, 5, axis=1)
valid_repeat = np.repeat(valid_label_logreg, 5, axis=1)
leng_arr = [len(valid_repeat), len(valid_repeat), len(valid_repeat), len(valid_repeat), len(valid_repeat)]

# Logistic Regression
# Updating the weights and calculating error rates for every iteration
# stochastic descent was used
for i in range(epoch_number):
  log_ite = np.dot(normalized_train_logreg,wgh_ite)
  sigmoid_ite = log_ite >= np.log(0.5)
  gra_ite = np.matmul((sigmoid_ite - train_repeat).T, normalized_train_logreg)
  wgh_ite = wgh_ite - lrn_rate * gra_ite.T
  estimation_ite = np.dot(normalized_valid_logreg,wgh_ite)
  sigmoid_ite_valid = estimation_ite>=np.log(0.5)
  np.count_nonzero(sigmoid_ite_valid - valid_repeat)
  accs[i,:] = (leng_arr - (np.count_nonzero((sigmoid_ite_valid - valid_repeat), axis = 0)))/len(valid_repeat)*100
  print(f'% Accuracy =', accs[i])

for i in range(len(accs.T)):
    plt.figure()
    plt.plot(accs[:,i])
    plt.xlabel("Number of Iterations")
    plt.ylabel("% Accuracy")
    plt.title(f"Validation Accuracy when Learning Rate is {lrn_rate[i]}")

wgh = np.array([0.0 for i in range(len(normalized_train_logreg[0]))])
wgh = wgh.reshape(np.size(wgh),1)
# print(np.shape(wgh))
# from validation set the best results are the following
learning_rate = 0.010
epoch = 2

# logistic regression
# log was used because exp overflowed
# stochastic descent was used
for i in range(epoch):
  log_odd = np.dot(normalized_train_logreg,wgh)
  sigmoid = log_odd >= np.log(0.5)
  # Stochastic Gradient Descent
  gradient = np.matmul((sigmoid - train_label_logreg).T, normalized_train_logreg)
  wgh = wgh - learning_rate * gradient.T

# Calculating the accuracy
log_estimation = np.dot(normalized_test_logreg,wgh)
sigmoid_test = log_estimation>=np.log(0.5)
# logreg_accuracy = (len(test_label_logreg) - (np.count_nonzero(sigmoid_test - test_label_logreg)))/len(test_label_logreg)*100
# print(f'Accuracy = {logreg_accuracy:.2f}%')
conf_matrix(test_label_logreg, sigmoid_test)

stop_timer_logreg_train = timeit.default_timer()
run_time_logreg_train = stop_timer_logreg_train - start_timer_logreg_train
print("The run time of the Logistic Regression is: ", run_time_logreg_train)

print(np.shape(normalized_train_logreg))
print(np.shape(normalized_test_logreg))
print(np.shape(normalized_valid_logreg))

"""# Decision Tree

"""

start_timer_decision_tree = timeit.default_timer()

# Data set for decision tree
data_train_df = pd.DataFrame(data_train_logreg)
list_clm = [i for i in range(10)]
data_train_df_reducted = data_train_df[list_clm]
data_train_df_reducted['round_winner'] = train_label_logreg
data_train_df_reducted

# Data set for decision tree for test
data_test_df = pd.DataFrame(data_test_logreg)
list_clm = [i for i in range(10)]
data_test_df_reducted = data_test_df[list_clm]
data_test_df_reducted['round_winner'] = test_label_logreg
data_test_df_reducted



#learning potential splits
def splits(data_train):
  # This functions learns the potential splits of the data that is given
  # param data_train: dataset whose potential splits will be found
  # return: splts for every possible splts in the data

  splts = {}
  rows, columns = np.shape(data_train)
  columns = columns -1

  for i in range(columns):
    row_values = data_train[:,i]
    unique_ones = np.unique(row_values)

    splts[i] = unique_ones
  return splts

#Classification of data
def data_classification(data_train):
  # This functions makes a classification for unique attributes
  # param data_train: dataset whose classification will be made
  # return: cls for max of the unique attributes

  data_label = data_train[:,-1]
  unique_attributes,count = np.unique(data_label,return_counts = True)

  ind = count.argmax()
  cls = unique_attributes[ind]

  return cls

def purity_checking(data_train):
  # This functions checks the purity of the data that is given as an input
  # param data_train: dataset whose purity will be checked
  # return: boolean that is true for pure data and false for not pure data
  data_label = data_train[:,-1]
  cls_uni = np.unique(data_label)

  len_cls_uni = len(cls_uni)

  if len_cls_uni == 1:
    return True
  else:
    return False

def split_function(data_train, column_of_split, value_of_split):
  # This functions splits the data according to given parameters
  # param data_train: dataset that is going to be split
  # return: less,more for data below and above of the data

  values_of_split_column = data_train[:, column_of_split]

  less = data_train[values_of_split_column <= value_of_split]
  more = data_train[values_of_split_column > value_of_split]

  return less, more

def entropy_calculation(data_train):
  # This functions calculates the entropy of the given data
  # param data_train: dataset whose entropy is going to be calculated
  # return: ent the result of entropy
  data_label = data_train[:,-1]
  unique_attributes,count = np.unique(data_label,return_counts = True)

  #calculating the entropy
  count_sum = count.sum()
  prob = count/count_sum
  ent = sum(prob*(-np.log2(prob)))

  return ent

def overall_etropy_calculation(less,more):
  # This functions calculates the overall entropy for data above and below
  # return: ent_ov for the overall entropy
  total_lenght = len(more) + len(less)

  weight_less = len(less) / (total_lenght)
  weight_more = len(more) / (total_lenght)

  ent_ov = (weight_less*entropy_calculation(less) + weight_more*entropy_calculation(more))

  return ent_ov

def the_best_split(data_train,splits):
  #This function finds the best split of the given data and its potential splits
  #param data_train: the data whose best splits are going to be found
  #param splits: potential splits of the given data
  #return best_column_of_split, best_value_of_split: these are the return values
  #that represents the best column and best value of split
  entropy_overall = 999999

  for i in splits:
    for j in splits[i]:
      less,more = split_function(data_train, i, j)

      entropy_cur = overall_etropy_calculation(less,more)

      if entropy_cur <= entropy_overall:
        entropy_overall = entropy_cur
        best_column_of_split = i
        best_value_of_split = j


  return best_column_of_split, best_value_of_split

def main_decision_tree(df_train,count, samp = 2, depth_max = 7):
  # This function is for the main code for the decision tree algrithm
  # It uses all of the helper functions for the decision tree
  # param df_train: It takes the data as data frame
  # pram samp: minimum number of samples are taken from the user
  # param depth_max: this parameter determines the maximum number of decision tree depth
  # return small_tree: It returns the tree stucture of the data

  if count == 0:
    global headers_col
    headers_col = df_train.columns
    dataset = df_train.values
  else:
    dataset = df_train

  len_of_data = len(dataset)
  if purity_checking(dataset) or (count == depth_max) or (len_of_data<samp):
    cls = data_classification(dataset)
    return cls
  else:
    count = count + 1


#
  splts = {}
  rows, columns = np.shape(dataset)
  columns = columns -1

  for i in range(columns):
    row_values = dataset[:,i]
    unique_ones = np.unique(row_values)

    splts[i] = unique_ones
#

  #pot_splits = splits(dataset)
  best_column_of_split, best_value_of_split = the_best_split(dataset, splts)
  more, less = split_function(dataset, best_column_of_split, best_value_of_split)

  name_of_feature = headers_col[best_column_of_split]
  q = " {} <= {}".format(best_column_of_split,best_value_of_split)
  small_tree = {q: []}
  yes = main_decision_tree(less, count)
  no = main_decision_tree(more, count)

  if yes == no:
    small_tree == yes

  else:

    small_tree[q].append(yes)
    small_tree[q].append(no)

  return small_tree

#adding the label column into the train data
#prejection_df['round_winner'] = train_label_logreg
#prejection_df

# Trying the main_decision_tree function and create a tree
tree = main_decision_tree(data_train_df_reducted.head(10000),0)
tree

ex = data_test_df_reducted.iloc[0]
ex

def classification(ex, train_tree):
  # classification function takes an example from the test part of the dataset and tree of the train set
  #
  #
  q = list(train_tree.keys())[0]
  name, operator_of_comparison, the_value = q.split()

 # len_of_train_tree = len(train_tree)
  if len(train_tree)==0 or len(train_tree[q])==0:
    ans = 0
  else:
    if ex[int(name)] <= float(the_value):
      ans = train_tree[q][0]
    else:
      ans = train_tree[q][1]

  if not isinstance(ans,dict):
    return ans
  else:
    res = ans
    return classification(ex,res)

classification(ex,tree)

#prejection_df_test["round_winner"]=test_label_logreg
#prejection_df_test
#prejection_df_test

def accuracy(data_df,train_tree):
  # This function calculates the accuracy
  data_df["class"] = data_df.apply(classification,axis = 1, args=(train_tree,))
  print(data_df.head())
  data_df["correct_class"] = data_df["class"] == data_df["round_winner"]

  acc = data_df["correct_class"].mean()

  return acc

accuracy = accuracy(data_test_df_reducted,tree)

print("The accuracy of the Decision Tree is: ", accuracy)

stop_timer_decision_tree = timeit.default_timer()
run_time_decision_tree = stop_timer_decision_tree - start_timer_naive_test
print("The run time of the Decision Tree is: ", run_time_decision_tree)